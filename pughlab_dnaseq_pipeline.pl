#!/usr/bin/env perl
### pughlab_dnaseq_pipeline.pl #####################################################################
use AutoLoader 'AUTOLOAD';
use strict;
use warnings;
use Carp;
use POSIX qw(strftime);
use Getopt::Std;
use Getopt::Long;
use File::Basename;
use File::Path qw(make_path);
use YAML qw(LoadFile);
use List::Util qw(any none);
use Data::Dumper;

my $cwd = dirname(__FILE__);
require "$cwd/scripts/utilities.pl";

####################################################################################################
# version       author	  	comment
# 1.0		sprokopec       script to run PughLab DNASeq pipeline

### USAGE ##########################################################################################
# pughlab_dnaseq_pipeline.pl -c tool_config.yaml -d data.yaml
#
# where:
#	- tool_config.yaml contains tool versions and parameters, output directory, reference
#	information, etc.
#	- data_config.yaml contains sample information (YAML file containing paths to FASTQ files,
#	generated by create_fastq_yaml.pl)

### SUBROUTINES ####################################################################################

### MAIN ###########################################################################################
sub main {
	my %args = (
		tool_config	=> undef,
		data_config	=> undef,
		step1		=> undef,
		step2		=> undef,
		step3		=> undef,
		step4		=> undef,
		step5		=> undef,
		cleanup		=> undef,
		cluster		=> undef,
		dry_run		=> undef,
		@_
		);

	my $tool_config = $args{tool_config};
	my $data_config = $args{data_config};

	### PREAMBLE ######################################################################################

	# load tool config
	my $tool_data = LoadFile($tool_config);
	my $date = strftime "%F", localtime;
	my $timestamp = strftime "%F_%H-%M-%S", localtime;

	# check for and/or create output directories
	my $output_directory = $tool_data->{output_dir};
	$output_directory =~ s/\/$//;
	my $log_directory = join('/', $output_directory, 'logs', 'run_DNA_pipeline_' . $timestamp);

	unless(-e $output_directory) { make_path($output_directory); }
	unless(-e $log_directory) { make_path($log_directory); }

	# start logging
	my $log_file = join('/', $log_directory, 'run_DNASeq_pipeline.log');
	open (my $log, '>', $log_file) or die "Could not open $log_file for writing.";

	print $log "---\n";
	print $log "Running PughLab DNA-Seq pipeline.\n";
	print $log "\n  Tool config used: $tool_config";
	print $log "\n  Sample config used: $data_config";
	print $log "\n  Output directory: $output_directory";
	print $log "\n---\n\n";

	my $seq_type = $tool_data->{seq_type};

	# indicate maximum time limit for parent jobs to wait
	my $max_time = '5-00:00:00';
	if ('wgs' eq $tool_data->{seq_type}) { $max_time = '21-00:00:00'; }
	if ('targeted' eq $tool_data->{seq_type}) { $max_time = '3-00:00:00'; }

	my $perl = 'perl/' . $tool_data->{perl_version};
	my $samtools = 'samtools/' . $tool_data->{samtools_version};

	# get optional HPC group
	my $hpc_group = defined($tool_data->{hpc_group}) ? "-A $tool_data->{hpc_group}" : undef;

	### MAIN ###########################################################################################

	my ($run_script, $fastqc_run_id, $trim_run_id);
	my ($bwa_run_id, $gatk_run_id, $contest_run_id, $qc_run_id, $coverage_run_id, $hc_run_id);
	my ($manta_run_id, $strelka_run_id);
	my ($mutect_run_id, $mutect2_run_id, $varscan_run_id, $msi_run_id, $pindel_run_id);
	my ($somaticsniper_run_id, $delly_run_id, $vardict_run_id, $gatk_cnv_run_id, $novobreak_run_id);
	my ($mops_run_id, $svict_run_id, $ichor_run_id, $ascat_run_id, $mavis_run_id, $report_run_id);

	my (@step1_job_ids, @step2_job_ids, @step3_job_ids, @step4_job_ids, @job_ids);
	my $current_dependencies = '';

	# prepare directory structure
	my $trim_directory = join('/', $output_directory, 'fastq_trimmed');
	my $fastqc_directory = join('/', $output_directory, 'fastqc');
	my $bwa_directory = join('/', $output_directory, 'BWA');
	my $gatk_directory = join('/', $output_directory, 'GATK');
	my $gatk_cnv_directory = join('/', $output_directory, 'GATK_CNV');
	my $qc_directory = join('/', $output_directory, 'BAMQC', 'SequenceMetrics');
	my $contest_directory = join('/', $output_directory, 'BAMQC', 'ContEst');
	my $coverage_directory = join('/', $output_directory, 'BAMQC', 'Coverage');
	my $hc_directory = join('/', $output_directory, 'HaplotypeCaller');
	my $manta_directory = join('/', $output_directory, 'Manta');
	my $strelka_directory = join('/', $output_directory, 'Strelka');
	my $msi_directory = join('/', $output_directory, 'MSI');
	my $mutect_directory = join('/', $output_directory, 'MuTect');
	my $mutect2_directory = join('/', $output_directory, 'MuTect2');
	my $varscan_directory = join('/', $output_directory, 'VarScan');
	my $vardict_directory = join('/', $output_directory, 'VarDict');
	my $pindel_directory = join('/', $output_directory, 'Pindel');
	my $somaticsniper_directory = join('/', $output_directory, 'SomaticSniper');
	my $delly_directory = join('/', $output_directory, 'Delly');
	my $novobreak_directory = join('/', $output_directory, 'NovoBreak');
	my $mops_directory = join('/', $output_directory, 'panelCNmops');
	my $svict_directory = join('/', $output_directory, 'SViCT');
	my $ichor_directory = join('/', $output_directory, 'IchorCNA');
	my $ascat_directory = join('/', $output_directory, 'ASCAT');
	my $mavis_directory = join('/', $output_directory, 'Mavis');

	# check which tools have been requested
	my %tool_set = (
		'trim_adapters' => defined($tool_data->{trim_adapters}->{run}) ? $tool_data->{trim_adapters}->{run} : 'N',
		'fastqc' => defined($tool_data->{fastqc}->{run}) ? $tool_data->{fastqc}->{run} : 'N',
		'bwa'	=> defined($tool_data->{bwa}->{run}) ? $tool_data->{bwa}->{run} : 'N',
		'gatk'	=> defined($tool_data->{gatk}->{run}) ? $tool_data->{gatk}->{run} : 'N',
		'bamqc'	=> defined($tool_data->{bamqc}->{run}) ? $tool_data->{bamqc}->{run} : 'N',
		'haplotype_caller' => defined($tool_data->{haplotype_caller}->{run}) ? $tool_data->{haplotype_caller}->{run} : 'N',
		'mutect'	=> defined($tool_data->{mutect}->{run}) ? $tool_data->{mutect}->{run} : 'N',
		'mutect2'	=> defined($tool_data->{mutect2}->{run}) ? $tool_data->{mutect2}->{run} : 'N',
		'somaticsniper'	=> defined($tool_data->{somaticsniper}->{run}) ? $tool_data->{somaticsniper}->{run} : 'N',
		'manta'		=> defined($tool_data->{manta}->{run}) ? $tool_data->{manta}->{run} : 'N',
		'strelka'	=> defined($tool_data->{strelka}->{run}) ? $tool_data->{strelka}->{run} : 'N',
		'varscan'	=> defined($tool_data->{varscan}->{run}) ? $tool_data->{varscan}->{run} : 'N',
		'vardict'	=> defined($tool_data->{vardict}->{run}) ? $tool_data->{vardict}->{run} : 'N',
		'pindel'	=> defined($tool_data->{pindel}->{run}) ? $tool_data->{pindel}->{run} : 'N',
		'gatk_cnv'	=> defined($tool_data->{gatk_cnv}->{run}) ? $tool_data->{gatk_cnv}->{run} : 'N',
		'novobreak'	=> defined($tool_data->{novobreak}->{run}) ? $tool_data->{novobreak}->{run} : 'N',
		'ichor_cna'	=> defined($tool_data->{ichor_cna}->{run}) ? $tool_data->{ichor_cna}->{run} : 'N',
		'ascat'	=> defined($tool_data->{ascat}->{run}) ? $tool_data->{ascat}->{run} : 'N',
		'delly'	=> defined($tool_data->{delly}->{run}) ? $tool_data->{delly}->{run} : 'N',
		'svict'	=> defined($tool_data->{svict}->{run}) ? $tool_data->{svict}->{run} : 'N',
		'mops'	=> defined($tool_data->{panelcn_mops}->{run}) ? $tool_data->{panelcn_mops}->{run} : 'N',
		'mavis'	=> defined($tool_data->{mavis}->{run}) ? $tool_data->{mavis}->{run} : 'N',
		'msi'	=> defined($tool_data->{msi_sensor}->{run}) ? $tool_data->{msi_sensor}->{run} : 'N'
		);

	# force manta to run if strelka is requested
	if ( ('Y' eq $tool_set{'strelka'}) && ('N' eq $tool_set{'manta'}) ) {
		print $log "Setting manta to run:Y as this is required for strelka.";
		$tool_set{'manta'} = 'Y';
		}

	print $log Dumper \%tool_set;
	print $log "\n";

	# indicate YAML files for processed BAMs
	my $fastq_trimmed_output_yaml = join('/', 
		$trim_directory, 
		'fastq_trimmed_config' . $timestamp . '.yaml'
		);
	my $bwa_output_yaml = join('/', $bwa_directory, 'bwa_bam_config_' . $timestamp . '.yaml');
	my $gatk_output_yaml = join('/', $gatk_directory, 'gatk_bam_config_' . $timestamp . '.yaml');

	# are we running step1 (alignments) or are BAMs provided as input?
	if ( (!$args{step1}) ) {
		$gatk_output_yaml = $data_config;
		}

	if ( (!$args{step4}) && ($args{step5}) ) {
		print $log "Can not make final report without summarizing output; setting --summarize to true";
		$args{step4} = 1;
		}

	# Should pre-processing (adapter trimming/alignment/GATK processing/QC) be performed?
	if ($args{step1}) {

		## run AdapterTrim pipeline
		if ('Y' eq $tool_set{'trim_adapters'}) {

			unless(-e $trim_directory) { make_path($trim_directory); }

			my $trim_command = join(' ',
				"perl $cwd/scripts/trim_adapters.pl",
				"-o", $trim_directory,
				"-t", $tool_config,
				"-d", $data_config,
				"-b", $fastq_trimmed_output_yaml,
				"-c", $args{cluster}
				);

			# record command (in log directory) and then run job
			print $log "Submitting job for trim_adapters.pl\n";
			print $log "  COMMAND: $trim_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_trim_adapters',
				cmd	=> $trim_command,
				modules	=> [$perl],
				mem		=> '256M',
				max_time	=> '48:00:00',
				hpc_driver	=> $args{cluster},
				extra_args	=> [$hpc_group]
				);

			if ($args{dry_run}) {

				$trim_command .= " --dry-run";
				`$trim_command`;
				$trim_run_id = 'pughlab_dna_pipeline__run_trim_adapters';

				} else {

				$trim_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> AdapterTrim job id: $trim_run_id\n\n";
				push @step1_job_ids, $trim_run_id;
				push @job_ids, $trim_run_id;
				}

			} else {
			$fastq_trimmed_output_yaml = $data_config;
			}

		## run FASTQC pipeline
		if ('Y' eq $tool_set{'fastqc'}) {

			unless(-e $fastqc_directory) { make_path($fastqc_directory); }

			my $fastqc_command = join(' ',
				"perl $cwd/scripts/collect_fastqc_metrics.pl",
				"-o", $fastqc_directory,
				"-t", $tool_config,
				"-d", $fastq_trimmed_output_yaml,
				"-c", $args{cluster}
				);

			# record command (in log directory) and then run job
			print $log "Submitting job for collect_fastqc_metrics.pl\n";
			print $log "  COMMAND: $fastqc_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_fastqc',
				cmd	=> $fastqc_command,
				modules	=> [$perl],
				dependencies	=> $trim_run_id,
				mem		=> '256M',
				max_time	=> '48:00:00',
				hpc_driver	=> $args{cluster},
				extra_args	=> [$hpc_group]
				);

			if ($args{dry_run}) {

				$fastqc_command .= " --dry-run";
				`$fastqc_command`;
				$fastqc_run_id = 'pughlab_dna_pipeline__run_fastqc';

				} else {

				$fastqc_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> FASTQC job id: $fastqc_run_id\n\n";
				push @step1_job_ids, $fastqc_run_id;
				push @job_ids, $fastqc_run_id;
				}
			}

		## run BWA-alignment pipeline
		if ('Y' eq $tool_set{'bwa'}) {

			unless(-e $bwa_directory) { make_path($bwa_directory); }

			my $bwa_command = join(' ',
				"perl $cwd/scripts/bwa.pl",
				"-o", $bwa_directory,
				"-t", $tool_config,
				"-d", $fastq_trimmed_output_yaml,
				"-b", $bwa_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$bwa_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for bwa.pl\n";
			print $log "  COMMAND: $bwa_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_bwa',
				cmd	=> $bwa_command,
				modules	=> [$perl],
				dependencies	=> $trim_run_id,
				mem		=> '256M',
				max_time	=> $max_time,
				hpc_driver	=> $args{cluster},
				extra_args	=> [$hpc_group]
				);

			if ($args{dry_run}) {

				$bwa_command .= " --dry-run";
				`$bwa_command`;
				$bwa_run_id = 'pughlab_dna_pipeline__run_bwa';

				} else {

				$bwa_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> BWA job id: $bwa_run_id\n\n";
				push @step1_job_ids, $bwa_run_id;
				push @job_ids, $bwa_run_id;
				}
			}

		## run GATK indel realignment/recalibration pipeline
		if ('Y' eq $tool_set{'gatk'}) {

			unless(-e $gatk_directory) { make_path($gatk_directory); }

			my $gatk_command = join(' ',
				"perl $cwd/scripts/gatk.pl",
				"-o", $gatk_directory,
				"-t", $tool_config,
				"-d", $bwa_output_yaml,
				"-c", $args{cluster},
				"-b", $gatk_output_yaml
				);

			if ($args{cleanup}) {
				$gatk_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for gatk.pl\n";
			print $log "  COMMAND: $gatk_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_gatk',
				cmd	=> $gatk_command,
				modules	=> [$perl],
				dependencies	=> $bwa_run_id,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$gatk_command .= " --dry-run";
				`$gatk_command`;
				$gatk_run_id = 'pughlab_dna_pipeline__run_gatk';

				} else {

				$gatk_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> GATK job id: $gatk_run_id\n\n";
				push @step1_job_ids, $gatk_run_id;
				push @job_ids, $gatk_run_id;
				}
			}
		}

	# make sure the dependency isn't empty
	if (scalar(@step1_job_ids) > 0) {
		$current_dependencies = join(':', @step1_job_ids);
		} else {
		$current_dependencies = '';
		}

	## run GATK's CalculateContamination and DepthOfCoverage
	## also collect alignment metrics, and calculate callable Bases
	if ($args{step2}) {

		unless(-e $qc_directory) { make_path($qc_directory); }
		unless(-e $coverage_directory) { make_path($coverage_directory); }

		if ('Y' eq $tool_set{'bamqc'}) {

			# ContEst (GATK v3.x) pipeline
			# this tool is now optional as the per-lane runs frequently fail and overall
			# contamination estimates are now provided as part of the general QC package below
			if ('Y' eq $tool_data->{bamqc}->{parameters}->{contest}->{run}) {

				unless(-e $contest_directory) { make_path($contest_directory); }

				my $contest_command = join(' ',
					"perl $cwd/scripts/contest.pl",
					"-o", $contest_directory,
					"-t", $tool_config,
					"-d", $gatk_output_yaml,
					"-c", $args{cluster}
					);

				if ($args{cleanup}) {
					$contest_command .= " --remove";
					}

				# record command (in log directory) and then run job
				print $log "Submitting job for contest.pl\n";
				print $log "  COMMAND: $contest_command\n\n";

				$run_script = write_script(
					log_dir	=> $log_directory,
					name	=> 'pughlab_dna_pipeline__run_contest',
					cmd	=> $contest_command,
					modules	=> [$perl],
					dependencies	=> join(':', @step1_job_ids),
					mem		=> '256M',
					max_time	=> $max_time,
					extra_args	=> [$hpc_group],
					hpc_driver	=> $args{cluster}
					);

				if ($args{dry_run}) {

					$contest_command .= " --dry-run";
					`$contest_command`;
					$contest_run_id = 'pughlab_dna_pipeline__run_contest';

					} else {

					$contest_run_id = submit_job(
						jobname		=> $log_directory,
						shell_command	=> $run_script,
						hpc_driver	=> $args{cluster},
						dry_run		=> $args{dry_run},
						log_file	=> $log
						);

					print $log ">>> ContEst job id: $contest_run_id\n\n";
					push @step2_job_ids, $contest_run_id;
					push @job_ids, $contest_run_id;
					}
				}

			# QC (GATK v4.x) pipeline
			my $qc_command = join(' ',
				"perl $cwd/scripts/get_sequencing_metrics.pl",
				"-o", $qc_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$qc_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for get_sequencing_metrics.pl\n";
			print $log "  COMMAND: $qc_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_qc',
				cmd	=> $qc_command,
				modules	=> [$perl],
				dependencies	=> join(':', @step1_job_ids),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$qc_command .= " --dry-run";
				`$qc_command`;
				$qc_run_id = 'pughlab_dna_pipeline__run_qc';

				} else {

				$qc_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> QC job id: $qc_run_id\n\n";
				push @step2_job_ids, $qc_run_id;
				push @job_ids, $qc_run_id;
				}

			# Coverage (GATK, any version) + callable bases pipeline
			my $coverage_command = join(' ',
				"perl $cwd/scripts/get_coverage.pl",
				"-o", $coverage_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$coverage_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for get_coverage.pl\n";
			print $log "  COMMAND: $coverage_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_coverage',
				cmd	=> $coverage_command,
				modules	=> [$perl],
				dependencies	=> join(':', @step1_job_ids),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$coverage_command .= " --dry-run";
				`$coverage_command`;
				$coverage_run_id = 'pughlab_dna_pipeline__run_coverage';

				} else {

				$coverage_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> Coverage job id: $coverage_run_id\n\n";
				push @step2_job_ids, $coverage_run_id;
				push @job_ids, $coverage_run_id;
				}
			}
		}

	# make sure the dependency isn't empty
	if (scalar(@step2_job_ids) > 0) {
		$current_dependencies = join(':', @step2_job_ids);
		} # else will leave as step1 jobs (or blank)

	########################################################################################
	# From here on out, it makes more sense to run everything as a single batch.
	########################################################################################
	if ($args{step3}) {

		## run GATK's HaplotypeCaller pipeline
		if ('Y' eq $tool_set{'haplotype_caller'}) {
	 
			$hc_run_id = '';
			my $hc_command = join(' ',
				"perl $cwd/scripts/haplotype_caller.pl",
				"-o", $hc_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$hc_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for haplotype_caller.pl\n";
			print $log "  COMMAND: $hc_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_haplotypecaller',
				cmd	=> $hc_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$hc_command .= " --dry-run";
				`$hc_command`;
				$hc_run_id = 'pughlab_dna_pipeline__run_haplotypecaller';

				} else {

				$hc_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> HaplotypeCaller job id: $hc_run_id\n\n";
				push @step3_job_ids, $hc_run_id;
				push @job_ids, $hc_run_id;
				}

			# next step will run Genotype GVCFs, and filter final output
			$hc_command = join(' ',
				"perl $cwd/scripts/genotype_gvcfs.pl",
				"-o", $hc_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$hc_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for genotype_gvcfs.pl\n";
			print $log "  COMMAND: $hc_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_genotype_gvcfs',
				cmd	=> $hc_command,
				modules	=> [$perl],
				dependencies	=> join(':', $current_dependencies, $hc_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);
		
			if ($args{dry_run}) {

				$hc_command .= " --dry-run";
				`$hc_command`;
				$hc_run_id = 'pughlab_dna_pipeline__run_genotype_gvcfs';

				} else {

				$hc_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> GenotypeGVCFs job id: $hc_run_id\n\n";
				push @job_ids, $hc_run_id;
				}

			# finally, annotate and filter using CPSR/PCGR
			$hc_command = join(' ',
				"perl $cwd/scripts/annotate_germline.pl",
				"-o", $hc_directory,
				"-i", join('/', $hc_directory, 'cohort'),
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$hc_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for annotate_germline.pl\n";
			print $log "  COMMAND: $hc_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_annotate_germline',
				cmd	=> $hc_command,
				modules	=> [$perl],
				dependencies	=> join(':', $current_dependencies, $hc_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);
		
			if ($args{dry_run}) {
				$hc_run_id = 'pughlab_dna_pipeline__run_annotate_germline';
				} else {
				$hc_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> AnnotateGermline job id: $hc_run_id\n\n";
				push @job_ids, $hc_run_id;
				}
			}

		## run MANTA pipeline
		if ('Y' eq $tool_set{'manta'}) {

			unless(-e $manta_directory) { make_path($manta_directory); }

			# next run somatic variant calling
			my $manta_command = join(' ',
				"perl $cwd/scripts/manta.pl",
				"-o", $manta_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$manta_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for manta.pl\n";
			print $log "  COMMAND: $manta_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_manta',
				cmd	=> $manta_command,
				modules	=> [$perl,$samtools],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$manta_command .= " --dry-run";
				`$manta_command`;
				$manta_run_id = 'pughlab_dna_pipeline__run_manta';

				} else {

				$manta_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> Manta job id: $manta_run_id\n\n";
				push @step3_job_ids, $manta_run_id;
				push @job_ids, $manta_run_id;
				}
			}

		## run STRELKA pipeline
		if ('Y' eq $tool_set{'strelka'}) {

			unless(-e $strelka_directory) { make_path($strelka_directory); }

			# first create a panel of normals
			my $pon = $tool_data->{strelka}->{pon};
			my $strelka_command;

			if (defined($tool_data->{strelka}->{pon})) {
				$strelka_run_id = '';
				} else {

				$pon = join('/', $strelka_directory, 'panel_of_normals.vcf');

				$strelka_command = join(' ',
					"perl $cwd/scripts/strelka.pl",
					"-o", $strelka_directory,
					"-t", $tool_config,
					"-d", $gatk_output_yaml,
					"--create-panel-of-normals",
					"-c", $args{cluster}
					);

				if ($args{cleanup}) {
					$strelka_command .= " --remove";
					}

				# record command (in log directory) and then run job
				print $log "Submitting job for strelka.pl --create-panel-of-normals\n";
				print $log "  COMMAND: $strelka_command\n\n";

				$run_script = write_script(
					log_dir	=> $log_directory,
					name	=> 'pughlab_dna_pipeline__run_strelka_pon',
					cmd	=> $strelka_command,
					modules	=> [$perl,$samtools],
					dependencies	=> $current_dependencies,
					mem		=> '256M',
					max_time	=> $max_time,
					extra_args	=> [$hpc_group],
					hpc_driver	=> $args{cluster}
					);

				if ($args{dry_run}) {

					$strelka_command .= " --dry-run";
					`$strelka_command`;
					$strelka_run_id = 'pughlab_dna_pipeline__run_strelka_pon';

					} else {

					$strelka_run_id = submit_job(
						jobname		=> $log_directory,
						shell_command	=> $run_script,
						hpc_driver	=> $args{cluster},
						dry_run		=> $args{dry_run},
						log_file	=> $log
						);

					print $log ">>> Strelka PoN job id: $strelka_run_id\n\n";
					push @step3_job_ids, $strelka_run_id;
					push @job_ids, $strelka_run_id;
					}
				}

			# next run somatic variant calling
			$strelka_command = join(' ',
				"perl $cwd/scripts/strelka.pl",
				"-o", $strelka_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster},
				"-m", $manta_directory,
				"--pon", $pon
				);

			if ($args{cleanup}) {
				$strelka_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for strelka.pl\n";
			print $log "  COMMAND: $strelka_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_strelka',
				cmd	=> $strelka_command,
				modules	=> [$perl,$samtools],
				dependencies	=> join(':', $current_dependencies, $strelka_run_id, $manta_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$strelka_command .= " --dry-run";
				`$strelka_command`;
				$strelka_run_id = 'pughlab_dna_pipeline__run_strelka';

				} else {

				$strelka_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> Strelka job id: $strelka_run_id\n\n";
				push @step3_job_ids, $strelka_run_id;
				push @job_ids, $strelka_run_id;
				}
			}

		## run GATK's MuTect pipeline
		if ('Y' eq $tool_set{'mutect'}) {

			unless(-e $mutect_directory) { make_path($mutect_directory); }
 
			my $pon = $tool_data->{mutect}->{pon};
			my $mutect_command;

			if (defined($tool_data->{mutect}->{pon})) {
				$mutect_run_id = '';
				} else {
				$pon = join('/', $mutect_directory, 'panel_of_normals.vcf');

				# first create a panel of normals
				$mutect_command = join(' ',
					"perl $cwd/scripts/mutect.pl",
					"-o", $mutect_directory,
					"-t", $tool_config,
					"-d", $gatk_output_yaml,
					"-c", $args{cluster},
					"--create-panel-of-normals"
					);

				if ($args{cleanup}) {
					$mutect_command .= " --remove";
					}

				# record command (in log directory) and then run job
				print $log "Submitting job for mutect.pl --create-panel-of-normals\n";
				print $log "  COMMAND: $mutect_command\n\n";

				$run_script = write_script(
					log_dir	=> $log_directory,
					name	=> 'pughlab_dna_pipeline__run_mutect_pon',
					cmd	=> $mutect_command,
					modules	=> [$perl],
					dependencies	=> $current_dependencies,
					mem		=> '256M',
					max_time	=> $max_time,
					extra_args	=> [$hpc_group],
					hpc_driver	=> $args{cluster}
					);

				if ($args{dry_run}) {

					$mutect_command .= " --dry-run";
					`$mutect_command`;
					$mutect_run_id = 'pughlab_dna_pipeline__run_mutect_pon';

					} else {

					$mutect_run_id = submit_job(
						jobname		=> $log_directory,
						shell_command	=> $run_script,
						hpc_driver	=> $args{cluster},
						dry_run		=> $args{dry_run},
						log_file	=> $log
						);
				
					print $log ">>> MuTect PoN job id: $mutect_run_id\n\n";
					push @step3_job_ids, $mutect_run_id;
					push @job_ids, $mutect_run_id;
					}
				}

			# next run somatic variant calling
			$mutect_command = join(' ',
				"perl $cwd/scripts/mutect.pl",
				"-o", $mutect_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster},
				"--pon", $pon
				);

			if ($args{cleanup}) {
				$mutect_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for mutect.pl\n";
			print $log "  COMMAND: $mutect_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_mutect',
				cmd	=> $mutect_command,
				modules	=> [$perl],
				dependencies	=> join(':', $current_dependencies, $mutect_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$mutect_command .= " --dry-run";
				`$mutect_command`;
				$mutect_run_id = 'pughlab_dna_pipeline__run_mutect';

				} else {

				$mutect_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> MuTect job id: $mutect_run_id\n\n";
				push @step3_job_ids, $mutect_run_id;
				push @job_ids, $mutect_run_id;
				}
			}

		## also run GATK's newer MuTect2 pipeline
		if ('Y' eq $tool_set{'mutect2'}) {

			unless(-e $mutect2_directory) { make_path($mutect2_directory); }

			my $pon = $tool_data->{mutect2}->{pon};
			my $mutect2_command;

			if (defined($tool_data->{mutect2}->{pon})) {
				$mutect2_run_id = '';
				} else {
				$pon = join('/', $mutect2_directory, 'panel_of_normals.vcf');

				# first create a panel of normals
				$mutect2_command = join(' ',
					"perl $cwd/scripts/mutect2.pl",
					"-o", $mutect2_directory,
					"-t", $tool_config,
					"-d", $gatk_output_yaml,
					"-c", $args{cluster},
					"--create-panel-of-normals"
					);

				if ($args{cleanup}) {
					$mutect2_command .= " --remove";
					}

				# record command (in log directory) and then run job
				print $log "Submitting job for mutect2.pl --create-panel-of-normals\n";
				print $log "  COMMAND: $mutect2_command\n\n";

				$run_script = write_script(
					log_dir	=> $log_directory,
					name	=> 'pughlab_dna_pipeline__run_mutect2_pon',
					cmd	=> $mutect2_command,
					modules	=> [$perl],
					dependencies	=> $current_dependencies,
					mem		=> '256M',
					max_time	=> $max_time,
					extra_args	=> [$hpc_group],
					hpc_driver	=> $args{cluster}
					);

				if ($args{dry_run}) {

					$mutect2_command .= " --dry-run";
					`$mutect2_command`;
					$mutect2_run_id = 'pughlab_dna_pipeline__run_mutect2_pon';

					} else {

					$mutect2_run_id = submit_job(
						jobname		=> $log_directory,
						shell_command	=> $run_script,
						hpc_driver	=> $args{cluster},
						dry_run		=> $args{dry_run},
						log_file	=> $log
						);
				
					print $log ">>> MuTect2 PoN job id: $mutect2_run_id\n\n";
					push @step3_job_ids, $mutect2_run_id;
					push @job_ids, $mutect2_run_id;
					}
				}

			# now run the actual somatic variant calling
			$mutect2_command = join(' ',
				"perl $cwd/scripts/mutect2.pl",
				"-o", $mutect2_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster},
				"--pon", $pon
				);

			if ($args{cleanup}) {
				$mutect2_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for mutect2.pl\n";
			print $log "  COMMAND: $mutect2_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_mutect2',
				cmd	=> $mutect2_command,
				modules	=> [$perl,$samtools],
				dependencies	=> join(':', $current_dependencies, $mutect2_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$mutect2_command .= " --dry-run";
				`$mutect2_command`;
				$mutect2_run_id = 'pughlab_dna_pipeline__run_mutect2';

				} else {

				$mutect2_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> MuTect2 job id: $mutect2_run_id\n\n";
				push @step3_job_ids, $mutect2_run_id;
				push @job_ids, $mutect2_run_id;
				}
			}

		## run VarScan SNV/CNV pipeline
		if ('Y' eq $tool_set{'varscan'}) {

			unless(-e $varscan_directory) { make_path($varscan_directory); }

			my $varscan_command = join(' ',
				"perl $cwd/scripts/varscan.pl",
				"-o", $varscan_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if (defined($tool_data->{varscan}->{pon})) {
				$varscan_command .= " --pon $tool_data->{varscan}->{pon}";
				}

			if ($args{cleanup}) {
				$varscan_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for varscan.pl\n";
			print $log "  COMMAND: $varscan_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_varscan',
				cmd	=> $varscan_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {
			
				$varscan_command .= " --dry-run";
				`$varscan_command`;
				$varscan_run_id = 'pughlab_dna_pipeline__run_varscan';

				} else {

				$varscan_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> VarScan job id: $varscan_run_id\n\n";
				push @step3_job_ids, $varscan_run_id;
				push @job_ids, $varscan_run_id;
				}

			# run sequenza pipeline
			my $sequenza_command = join(' ',
				"perl $cwd/scripts/run_sequenza_with_optimal_gamma.pl",
				"-o", $varscan_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			# record command (in log directory) and then run job
			print $log "Submitting job for run_sequenza_with_optimal_gamma.pl\n";
			print $log "  COMMAND: $sequenza_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_sequenza',
				cmd	=> $sequenza_command,
				modules	=> [$perl],
				dependencies	=> join(':', $current_dependencies, $varscan_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {
				$varscan_run_id = 'pughlab_dna_pipeline__run_sequenza';
				} else {			
				$varscan_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> Sequenza job id: $varscan_run_id\n\n";
				push @job_ids, $varscan_run_id;
				}
			}

		## SomaticSniper pipeline
		if ('Y' eq $tool_set{'somaticsniper'}) {

			unless(-e $somaticsniper_directory) { make_path($somaticsniper_directory); }

			my $somaticsniper_command = join(' ',
				"perl $cwd/scripts/somaticsniper.pl",
				"-o", $somaticsniper_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$somaticsniper_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for somaticsniper.pl\n";
			print $log "  COMMAND: $somaticsniper_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_somaticsniper',
				cmd	=> $somaticsniper_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$somaticsniper_command .= " --dry-run";
				`$somaticsniper_command`;
				$somaticsniper_run_id = 'pughlab_dna_pipeline__run_somaticsniper';

				} else {

				$somaticsniper_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> SomaticSniper job id: $somaticsniper_run_id\n\n";
				push @step3_job_ids, $somaticsniper_run_id;
				push @job_ids, $somaticsniper_run_id;
				}
			}

		## GATK-CNV pipeline
		if ('Y' eq $tool_set{'gatk_cnv'}) {

			unless(-e $gatk_cnv_directory) { make_path($gatk_cnv_directory); }

			my $gatk_cnv_command = join(' ',
				"perl $cwd/scripts/gatk_cnv.pl",
				"-o", $gatk_cnv_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			#if ($args{cleanup}) {
			#	$gatk_cnv_command .= " --remove";
			#	}

			# record command (in log directory) and then run job
			print $log "Submitting job for gatk_cnv.pl\n";
			print $log "  COMMAND: $gatk_cnv_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_gatk_cnv',
				cmd	=> $gatk_cnv_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$gatk_cnv_command .= " --dry-run";
				`$gatk_cnv_command`;
				$gatk_cnv_run_id = 'pughlab_dna_pipeline__run_gatk_cnv';

				} else {

				$gatk_cnv_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> GATK-CNV job id: $gatk_cnv_run_id\n\n";
				push @step3_job_ids, $gatk_cnv_run_id;
				push @job_ids, $gatk_cnv_run_id;
				}
			}

		# let's let the first bunch finish before starting the next set
		if (scalar(@step3_job_ids) > 4) {
			$current_dependencies = join(':', @step3_job_ids);
			}

		## SViCT pipeline
		if ('Y' eq $tool_set{'svict'}) {

			unless(-e $svict_directory) { make_path($svict_directory); }

			my $svict_command = join(' ',
				"perl $cwd/scripts/svict.pl",
				"-o", $svict_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$svict_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for svict.pl\n";
			print $log "  COMMAND: $svict_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_svict',
				cmd	=> $svict_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$svict_command .= " --dry-run";
				`$svict_command`;
				$svict_run_id = 'pughlab_dna_pipeline__run_svict';

				} else {

				$svict_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> SViCT job id: $svict_run_id\n\n";
				push @step4_job_ids, $svict_run_id;
				push @job_ids, $svict_run_id;
				}
			}

		## Pindel pipeline
		if ('Y' eq $tool_set{'pindel'}) {

			unless(-e $pindel_directory) { make_path($pindel_directory); }

			my $pindel_command = join(' ',
				"perl $cwd/scripts/pindel.pl",
				"-o", $pindel_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$pindel_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for pindel.pl\n";
			print $log "  COMMAND: $pindel_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_pindel',
				cmd	=> $pindel_command,
				modules	=> [$perl, $samtools],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$pindel_command .= " --dry-run";
				`$pindel_command`;
				$pindel_run_id = 'pughlab_dna_pipeline__run_pindel';

				} else {

				$pindel_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> Pindel job id: $pindel_run_id\n\n";
				push @step4_job_ids, $pindel_run_id;
				push @job_ids, $pindel_run_id;
				}
			}

		## panelCN.mops pipeline
		if ('Y' eq $tool_set{'mops'}) {

			unless(-e $mops_directory) { make_path($mops_directory); }

			my $mops_command = join(' ',
				"perl $cwd/scripts/panelcn_mops.pl",
				"-o", $mops_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$mops_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for panelcn_mops.pl\n";
			print $log "  COMMAND: $mops_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_panelcn_mops',
				cmd	=> $mops_command,
				modules	=> [$perl, $samtools],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$mops_command .= " --dry-run";
				`$mops_command`;
				$mops_run_id = 'pughlab_dna_pipeline__run_panelcn_mops';

				} else {

				$mops_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> panelCN_mops job id: $mops_run_id\n\n";
				push @step4_job_ids, $mops_run_id;
				push @job_ids, $mops_run_id;
				}
			}

		## IchorCNA pipeline
		if ('Y' eq $tool_set{'ichor_cna'}) {

			unless(-e $ichor_directory) { make_path($ichor_directory); }

			my $ichor_command = join(' ',
				"perl $cwd/scripts/ichor_cna.pl",
				"-o", $ichor_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$ichor_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for ichor_cna.pl\n";
			print $log "  COMMAND: $ichor_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_ichor_cna',
				cmd	=> $ichor_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$ichor_command .= " --dry-run";
				`$ichor_command`;
				$ichor_run_id = 'pughlab_dna_pipeline__run_ichor_cna';

				} else {

				$ichor_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> IchorCNA job id: $ichor_run_id\n\n";
				push @step4_job_ids, $ichor_run_id;
				push @job_ids, $ichor_run_id;
				}
			}

		## Ascat pipeline
		if ('Y' eq $tool_set{'ascat'}) {

			unless(-e $ascat_directory) { make_path($ascat_directory); }

			my $ascat_command = join(' ',
				"perl $cwd/scripts/ascat.pl",
				"-o", $ascat_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$ascat_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for ascat.pl\n";
			print $log "  COMMAND: $ascat_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_ascat',
				cmd	=> $ascat_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$ascat_command .= " --dry-run";
				`$ascat_command`;
				$ascat_run_id = 'pughlab_dna_pipeline__run_ascat';

				} else {

				$ascat_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> AscatCNA job id: $ascat_run_id\n\n";
				push @step4_job_ids, $ascat_run_id;
				push @job_ids, $ascat_run_id;
				}
			}

		## run Delly SV pipeline
		if ('Y' eq $tool_set{'delly'}) {

			unless(-e $delly_directory) { make_path($delly_directory); }

			my $delly_command = join(' ',
				"perl $cwd/scripts/delly.pl",
				"-o", $delly_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$delly_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for delly.pl\n";
			print $log "  COMMAND: $delly_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_delly',
				cmd	=> $delly_command,
				modules	=> [$perl, $samtools],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$delly_command .= " --dry-run";
				`$delly_command`;
				$delly_run_id = 'pughlab_dna_pipeline__run_delly';

				} else {

				$delly_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> Delly job id: $delly_run_id\n\n";
				push @step4_job_ids, $delly_run_id;
				push @job_ids, $delly_run_id;
				}
			}

		## MSI-Sensor pipeline
		if ('Y' eq $tool_set{'msi'}) {

			unless(-e $msi_directory) { make_path($msi_directory); }

			my $msi_command = join(' ',
				"perl $cwd/scripts/msi_sensor.pl",
				"-o", $msi_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$msi_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for msi_sensor.pl\n";
			print $log "  COMMAND: $msi_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_msi',
				cmd	=> $msi_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$msi_command .= " --dry-run";
				`$msi_command`;
				$msi_run_id = 'pughlab_dna_pipeline__run_msi';

				} else {

				$msi_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> MSI-Sensor job id: $msi_run_id\n\n";
				push @job_ids, $msi_run_id;
				}
			}

		# let's let the first bunch finish before starting the next set
		push @step3_job_ids, @step4_job_ids;
		if (scalar(@step4_job_ids) > 4) {
			$current_dependencies = join(':', @step4_job_ids);
			} elsif (scalar(@step3_job_ids) > 4) {
			$current_dependencies = join(':', @step3_job_ids);
			}

		## VarDict pipeline
		$vardict_run_id = '';

		if ('Y' eq $tool_set{'vardict'}) {

			unless(-e $vardict_directory) { make_path($vardict_directory); }

			my $vardict_command = "perl $cwd/scripts/vardict.pl";
			if ('wgs' eq $seq_type) { $vardict_command = "perl $cwd/scripts/vardict_wgs.pl"; }

			$vardict_command .= ' '. join(' ',
				"-o", $vardict_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if (defined($tool_data->{vardict}->{pon})) {
				$vardict_command .= " --pon $tool_data->{vardict}->{pon}";
				}

			if ($args{cleanup}) {
				$vardict_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for vardict.pl\n";
			print $log "  COMMAND: $vardict_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_vardict',
				cmd	=> $vardict_command,
				modules	=> [$perl],
				dependencies	=> $current_dependencies,
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$vardict_command .= " --dry-run";
				`$vardict_command`;
				$vardict_run_id = 'pughlab_dna_pipeline__run_vardict';

				} else {

				$vardict_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> VarDict job id: $vardict_run_id\n\n";
				push @step3_job_ids, $vardict_run_id;
				push @job_ids, $vardict_run_id;
				}
			}

		## NovoBreak pipeline
		if ('Y' eq $tool_set{'novobreak'}) {

			unless(-e $novobreak_directory) { make_path($novobreak_directory); }

			my $novobreak_command = "perl $cwd/scripts/novobreak.pl";

			$novobreak_command .= ' '. join(' ',
				"-o", $novobreak_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$novobreak_command .= " --remove";
				}

			# record command (in log directory) and then run job
			print $log "Submitting job for novobreak.pl\n";
			print $log "  COMMAND: $novobreak_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_novobreak',
				cmd	=> $novobreak_command,
				modules	=> [$perl],
				dependencies	=> join(':', $current_dependencies, $vardict_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {

				$novobreak_command .= " --dry-run";
				`$novobreak_command`;
				$novobreak_run_id = 'pughlab_dna_pipeline__run_novobreak';

				} else {

				$novobreak_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> NovoBreak job id: $novobreak_run_id\n\n";
				push @step3_job_ids, $novobreak_run_id;
				push @job_ids, $novobreak_run_id;
				}
			}

		## run Mavis SV annotation pipeline
		if ('Y' eq $tool_set{'mavis'}) {

			unless(-e $mavis_directory) { make_path($mavis_directory); }

			my $mavis_command = join(' ',
				"perl $cwd/scripts/mavis.pl",
				"-o", $mavis_directory,
				"-t", $tool_config,
				"-d", $gatk_output_yaml,
				"-c", $args{cluster}
				);

			if ($args{cleanup}) {
				$mavis_command .= " --remove";
				}

			# because mavis.pl will search provided directories and run based on what it finds
			# (Manta/Delly/NovoBreak/Pindel/SViCT resuts), this can only be run AFTER these
			# respective jobs finish
			my @depends;
			if (defined($delly_run_id)) {
				$mavis_command .= " --delly $delly_directory";
				push @depends, $delly_run_id;
				}
			if (defined($manta_run_id)) {
				$mavis_command .= " --manta $manta_directory";
				push @depends, $manta_run_id;
				}
			if (defined($novobreak_run_id)) {
				$mavis_command .= " --novobreak $novobreak_directory";
				push @depends, $novobreak_run_id;
				}
			# for wgs, because pindel is run in chunks, we miss the large SVs
			# so won't include it here
			if (defined($pindel_run_id) & ('wgs' ne $tool_data->{seq_type})) {
				$mavis_command .= " --pindel $pindel_directory";
				push @depends, $pindel_run_id;
				}
			if (defined($svict_run_id)) {
				$mavis_command .= " --svict $svict_directory";
				push @depends, $svict_run_id;
				}

			# if VarDict was run, add it as a dependency to avoid overwhelming the cluster
		#	if (defined($vardict_run_id)) {
		#		push @depends, $vardict_run_id;
		#		}

			# record command (in log directory) and then run job
			print $log "Submitting job for mavis.pl\n";
			print $log "  COMMAND: $mavis_command\n\n";

			$run_script = write_script(
				log_dir	=> $log_directory,
				name	=> 'pughlab_dna_pipeline__run_mavis',
				cmd	=> $mavis_command,
				modules	=> [$perl],
				dependencies	=> join(':', @depends, $vardict_run_id),
				mem		=> '256M',
				max_time	=> $max_time,
				extra_args	=> [$hpc_group],
				hpc_driver	=> $args{cluster}
				);

			if ($args{dry_run}) {
				$mavis_run_id = 'pughlab_dna_pipeline__run_mavis';
				} else {
				$mavis_run_id = submit_job(
					jobname		=> $log_directory,
					shell_command	=> $run_script,
					hpc_driver	=> $args{cluster},
					dry_run		=> $args{dry_run},
					log_file	=> $log
					);

				print $log ">>> MAVIS job id: $mavis_run_id\n\n";
				push @step3_job_ids, $mavis_run_id;
				push @job_ids, $mavis_run_id;
				}
			}
		}

	########################################################################################
	# Create a final report for the project.
	########################################################################################
	if ($args{step4}) {

		my $report_command = join(' ',
			"perl $cwd/scripts/pughlab_pipeline_auto_report.pl",
			"-t", $tool_config,
			"-c", $args{cluster},
			"-d", $gatk_output_yaml
			);

		if ($args{step5}) {
			$report_command .= " --create_report";
			}

		# record command (in log directory) and then run job
		print $log "Submitting job for pughlab_pipeline_auto_report.pl\n";
		print $log "  COMMAND: $report_command\n\n";

		$run_script = write_script(
			log_dir	=> $log_directory,
			name	=> 'pughlab_dna_pipeline__summarize_output',
			cmd	=> $report_command,
			modules	=> [$perl],
			dependencies	=> join(':', @step2_job_ids, @step3_job_ids),
			mem		=> '256M',
			max_time	=> '5-00:00:00',
			extra_args	=> [$hpc_group],
			hpc_driver	=> $args{cluster}
			);

		if ($args{dry_run}) {
			$report_run_id = 'pughlab_dna_pipeline__summarize_output';
			} else {
			$report_run_id = submit_job(
				jobname		=> $log_directory,
				shell_command	=> $run_script,
				hpc_driver	=> $args{cluster},
				dry_run		=> $args{dry_run},
				log_file	=> $log
				);

			print $log ">>> Report job id: $report_run_id\n\n";
			}
		}

	# finish up
	print $log "\nProgramming terminated successfully.\n\n";
	close $log;

	}

### GETOPTS AND DEFAULT VALUES #####################################################################
# declare variables
my ($tool_config, $data_config);
my ($preprocessing, $qc, $variant_calling, $summarize, $create_report);
my $hpc_driver = 'slurm';
my ($remove_junk, $dry_run);
my $help;

# read in command line arguments
GetOptions(
	'h|help'		=> \$help,
	't|tool=s'		=> \$tool_config,
	'd|data=s'		=> \$data_config,
	'preprocessing'		=> \$preprocessing,
	'qc'			=> \$qc,
	'variant_calling'	=> \$variant_calling,
	'summarize'		=> \$summarize,
	'create_report'		=> \$create_report,
	'c|cluster=s'		=> \$hpc_driver,
	'remove'		=> \$remove_junk,
	'dry-run'		=> \$dry_run
	 );

if ($help) {
	my $help_msg = join("\n",
		"Options:",
		"\t--help|-h\tPrint this help message",
		"\t--data|-d\t<string> data config (yaml format)",
		"\t--tool|-t\t<string> tool config (yaml format)",
		"\t--preprocessing\t<boolean> should data pre-processing be performed? (default: false)",
		"\t--qc\t<boolean> should QC metrics be generated on the BAMs? (default: false)",
		"\t--variant_calling\t<boolean> should variant calling be performed? (default: false)",
		"\t--summarize\t<boolean> should output be summarized? (default: false)",
		"\t--create_report\t<boolean> should a report be generated? (default: false)",
		"\t--cluster|-c\t<string> cluster scheduler (default: slurm)",
		"\t--remove\t<boolean> should intermediates be removed? (default: false)",
		"\t--dry-run\t<boolean> should jobs be submitted? (default: false)"
		);

	print "$help_msg\n";
	exit;
	}

if ( (!$preprocessing) && (!$variant_calling) && (!$summarize) && (!$qc) && (!$create_report) ) {
	die("Please choose a step to run (at least one of --preprocessing, --qc, --variant_calling, --summarize, --create_report )");
	}
if (!defined($tool_config)) {
	die("No tool config file defined; please provide -t | --tool (ie, tool_config.yaml)");
	}
if (!defined($data_config)) {
	die("No data config file defined; please provide -d | --data (ie, sample_config.yaml)");
	}

# check for compatible HPC driver; if not found, change dry_run to Y
my @compatible_drivers = qw(slurm);
if ( (none { $_ =~ m/$hpc_driver/ } @compatible_drivers ) && (!$dry_run) ) {
	print "Unrecognized HPC driver requested: setting dry_run to true -- jobs will not be submitted but commands will be written to file.\n";
	$dry_run = 1;
	}

main(
	tool_config	=> $tool_config,
	data_config	=> $data_config,
	step1		=> $preprocessing,
	step2		=> $qc,
	step3		=> $variant_calling,
	step4		=> $summarize,
	step5		=> $create_report,
	cluster		=> $hpc_driver,
	cleanup		=> $remove_junk,
	dry_run		=> $dry_run
	);
